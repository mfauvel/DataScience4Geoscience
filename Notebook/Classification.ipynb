{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "In this notebook, we will apply several classification algorithms on two data set: *Iris* and *Digits*. The first one is a low dimensional data set while the second one contains more features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits, load_iris\n",
    "# Load digits\n",
    "X, y = load_digits(return_X_y=True) # load_iris(return_X_y=True)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of the digits data set\n",
    "The data set is made available *by NIST to extract normalized bitmaps of handwritten digits from a preprinted form. From a total of 43 people, 30 contributed to the training set and different 13 to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of 4x4 and the number of on pixels are counted in each block. This generates an input matrix of 8x8 where each element is an integer in the range 0..16* (https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits)\n",
    "\n",
    "It is possible to visualize the data by reshaping correctly each sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the first element\n",
    "plt.figure()\n",
    "plt.imshow(X[0,:].reshape(8,8),cmap=\"gray\")\n",
    "plt.title(\"Label = {}\".format(y[0]))\n",
    "\n",
    "# Plot the hundredth element\n",
    "plt.figure()\n",
    "plt.imshow(X[100,:].reshape(8,8),cmap=\"gray\")\n",
    "plt.title(\"Label = {}\".format(y[100]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification framework\n",
    "Two steps are required\n",
    "- Model selection, i.e. find the optimal hyperparemeters,\n",
    "- Model assessement, i.e. validate the model on unseen data.\n",
    "\n",
    "As said in introduction, scikit-learn offers convenient and generic functions to achieve these steps. In what follow, an example is given for SVM. But it can be extended for any algorithm in scikit-learn, up to a correct definition of the hyperparameters. In this labworks, we use the support vector classifier (SVC) (http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Standardize data\n",
    "sc = MinMaxScaler()\n",
    "X = sc.fit_transform(X) # Scale data between 0 and 1\n",
    "\n",
    "# Split the data -> train_size=0.20 means we keep 20% of the data for training and 80% fir validation\n",
    "# The stratification ensures that the proportion of each class from the orginal data is preserved in the train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.20, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With kernel methods (and more generaly for any distance based methods), it is a good practice to standardize feature remove dynamics effect. Here we rescale each feature between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Definition of the hyperparameters values search\n",
    "degree = sp.arange(1,7) # Degree of the polynomial kernel\n",
    "C = 10.0**sp.arange(0,4) # Penality of the optimization problem\n",
    "param_grid = dict(degree=degree, C=C, kernel=['poly'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use [cross-validation](http://scikit-learn.org/stable/modules/cross_validation.html) strategy to select the optimal set of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid = GridSearchCV(SVC(),  # Set up the classifier\n",
    "                    param_grid=param_grid, \n",
    "                    cv= 3,\n",
    "                    n_jobs=-1) # Do the grid search in parallel\n",
    "grid.fit(X_train, y_train) # Run the grid search\n",
    "\n",
    "# Print some results\n",
    "print(\"Best score: {}\".format(grid.best_score_)) # Default scorer in scikit is the correct classification rate\n",
    "print(\"Best set of hyperparameters: {}\".format(grid.best_params_))\n",
    "\n",
    "# Learn the optimal model\n",
    "clf = grid.best_estimator_  # Get the best estimator\n",
    "clf.fit(X_train,y_train)  # Fit it using the training set\n",
    "\n",
    "# Predict new samples\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Compute the overall accuracies\n",
    "print(\"Correct classification rate on the test set: {}\".format(accuracy_score(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the correct classification rate function of the hyperparameters. It is important to check if our search values are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = grid.cv_results_['mean_test_score'].reshape(C.size,degree.size)\n",
    "X, Y = sp.meshgrid(degree, C)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "cp = ax.contourf(X, Y, res)\n",
    "ax.scatter(grid.best_params_['degree'],grid.best_params_['C'],color='k')\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_xlabel(\"Degree\")\n",
    "ax.set_ylabel(\"C\")\n",
    "fig.colorbar(cp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose we would like to compare several kernels. We need can use the same workflow, just by adding one line and modifying one line !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gamma = 2.0**sp.arange(-4,2) # Scale of the RBF kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_grid = [dict(kernel=['rbf'],gamma=gamma, C=C),\n",
    "              dict(kernel=['poly'],degree=degree, C=C)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now now copy/past/run the same code than previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid = GridSearchCV(SVC(),  # Set up the classifier\n",
    "                    param_grid=param_grid, \n",
    "                    cv= 3,\n",
    "                    n_jobs=-1) # Do the grid search in parallel\n",
    "\n",
    "grid.fit(X_train, y_train) # Run the grid search\n",
    "print(\"Best score: {}\".format(grid.best_score_))\n",
    "print(\"Best set of hyperparameters: {}\".format(grid.best_params_))\n",
    "print\n",
    "# Learn the optimal model\n",
    "clf = grid.best_estimator_  # Get the best estimator\n",
    "clf.fit(X_train,y_train)  # Fit it using the training set\n",
    "\n",
    "# Predict new samples\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Compute the overall accuracies\n",
    "print(\"Correct classification rate on the test set: {}\".format(accuracy_score(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of the differents classifiers\n",
    "In the course, we have discussed about three classifiers: QDA, SVM and K-NN. Using the following scripts, we are going to compare their performances on the given data sets. Again, using scikit-learn generic function, it is possible we few lines of code to run all the experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# This lines are used to remove warning from scikit-learn when QDA is used, to prevent noisy verbosity in the notebook: should be used in practice!\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Define the classifier list\n",
    "classifiers = [SVC(), KNeighborsClassifier(), QuadraticDiscriminantAnalysis(), LinearDiscriminantAnalysis(solver=\"lsqr\")]\n",
    "names = [\"SVM\", \"KNN\", \"QDA\", \"LDA\"]\n",
    "\n",
    "# Define the dictionnary of parameters to optimize\n",
    "param_grids = [dict(kernel=['rbf'], gamma=gamma, C=C),\n",
    "               dict(n_neighbors = sp.arange(1,40)), # number of neighbors for KNN\n",
    "               dict(reg_param = sp.linspace(0,0.1,30)), # Regularization parameter for QDA\n",
    "               dict(shrinkage = sp.linspace(0,0.5,30)), # Regularization parameter for LDA\n",
    "               ]\n",
    " \n",
    "# Run all classifiers\n",
    "for classifier, name, param_grid in zip(classifiers, names, param_grids):\n",
    "    grid = GridSearchCV(classifier, param_grid=param_grid, cv= 3, n_jobs=-1)\n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    clf = grid.best_estimator_ \n",
    "    clf.fit(X_train,y_train)\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(\"Classification accuracy for {}: {} (best parameters {})\".format(name,accuracy_score(y_test,y_pred),grid.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Pipeline\n",
    "In the previous section, we have selected the best kernel from the polynomial and RBF one. But it is also possible to combine them using [pipeline](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline) facility offer by scikit-learn. The following code does the job. \n",
    "\n",
    "We first define the CompositeKernel class, which is the weighted summation of the RBF and polynomial kernel with their own parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import rbf_kernel, polynomial_kernel\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Convenient Class for summation kernel\n",
    "class CompositeKernel(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self,\n",
    "                 mu=0.5,\n",
    "                 gamma=0.125,  \n",
    "                 degree=3):\n",
    "        self.gamma = gamma\n",
    "        self.degree = degree\n",
    "        self.mu = mu\n",
    "        \n",
    "    def transform(self,X):\n",
    "        K = self.mu*rbf_kernel(X,self.X_,gamma=self.gamma)\n",
    "        K += (1-self.mu)*polynomial_kernel(X,self.X_,degree=self.degree)\n",
    "        return K\n",
    "\n",
    "    def fit(self,X,y=None, **fit_params):\n",
    "        self.X_ = X\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a pipeline\n",
    "pipe = Pipeline([\n",
    "    ('CK',CompositeKernel()),\n",
    "    ('SVM',SVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define the range search for the weight paramter *mu* and pass the others parameters ranges. Here we are going to optimize all the parameters using grid search, i.e., a  brut force strategy. It exists several algorithms to do it properly, e.g., https://arxiv.org/abs/1602.02355. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_grid = dict([\n",
    "    ('CK__mu', sp.linspace(0,1,num=11)),  # Note the double \"_\" to define parameters of a pipeline\n",
    "    ('CK__gamma', gamma),\n",
    "    ('CK__degree', degree),\n",
    "    ('SVM__C',C),\n",
    "    ('SVM__kernel', ['precomputed']),\n",
    "])\n",
    "grid = GridSearchCV(pipe,  # Set up the classifier -> Here we put our pipeline\n",
    "                    param_grid=param_grid, \n",
    "                    cv= 3,\n",
    "                    n_jobs=-1) # Do the grid search in parallel\n",
    "\n",
    "grid.fit(X_train, y_train) # Run the grid search and WAIT: 11*6*6*4 tuples of values to be tested !\n",
    "print(\"Best score: {}\".format(grid.best_score_))\n",
    "print(\"Best set of hyperparameters: {}\".format(grid.best_params_))\n",
    "\n",
    "# Learn the optimal model\n",
    "clf = grid.best_estimator_  # Get the best estimator\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "# Predict new samples\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Compute the overall accuracies\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
